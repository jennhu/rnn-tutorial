{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks (RNNs) have become increasingly popular in machine learning for handling sequential data. In this tutorial, we will cover background about the architecture, a toy training example, and some demos for evaluating larger pre-trained models.\n",
    "\n",
    "**Date**: June 26, 2019\n",
    "\n",
    "**Authors**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "To run this tutorial, you will need to install [Pytorch](https://pytorch.org/). Just follow the instructions in the docs to install the library and its dependencies. To test your installation, go ahead and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "Note: The following figures were taken from excellent blog posts by [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Jianqiang Ma](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e), and [Shi Yan](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714).\n",
    "\n",
    "### 1.1 What is an RNN?\n",
    "\n",
    "In both machine learning and everyday life, many tasks make use of **sequential** data. Take language understanding, for example. As you comprehend each word in this sentence, you draw upon information from the previous words. RNNs have an **inductive bias** for handling this type of data, as parameters are shared across positions in the sequence.\n",
    "\n",
    "How does this architecture compare to \"normal\" neural networks? Let $A$ be a neural network, $x_t$ the input, and $h_t$ the output or hidden state vector. Consider the following diagram (credit to [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
    "\n",
    "![RNN](img/rnn_unrolled.png)\n",
    "\n",
    "The self-loop at $A$ represents the **recurrence** of the network. We can \"unroll\" it into multiple copies of the same network $A$, sending information down the temporal chain.\n",
    "\n",
    "In practice, many applications use a special type of RNN called a **long short-term memory (LSTM)** network. We'll call non-LSTM RNNs \"vanilla RNNs\".\n",
    "\n",
    "### 1.2 What is an LSTM?\n",
    "\n",
    "To motivate the popularity of LSTMs, recall why we were interested in RNNs in the first place: modeling *sequences*. The idea is that items at later parts of a sequence can depend on items at earlier parts of the sequence. The distance between these related items is often called the **dependency length**.\n",
    "\n",
    "Dependency lengths can be quite short in some cases and quite long in others, even for the same task. Returning to the language example, suppose your task is to predict the last word in a sentence. If your sentence is \n",
    "\n",
    "> A car has four *wheels*.\n",
    "\n",
    "then the gap between the target word (here, \"wheels\") and the relevant parts of the sequence (here, \"car\") is relatively small. In contrast, consider the sentence \n",
    "\n",
    "> A car can have sentimental value for many owners for a variety of reasons, and can come in many models, sizes, and colors; nevertheless, one defining characteristic of such a machine is that it has four *wheels*.\n",
    "\n",
    "Here, the gap between \"wheels\" and \"car\" is relatively large. Words closer to the end of the sentence such as \"machine\" and \"four\" can help guide the prediction of \"wheels\", but we need the word \"car\" to nail down the correct word.\n",
    "\n",
    "In theory, vanilla RNNs are able to capture these long-term dependencies, but this doesn't work well in practice (see [this paper](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) and [this paper](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf)). This is where LSTMs come in.\n",
    "\n",
    "#### 1.2.1 LSTM architecture\n",
    "\n",
    "![LSTM](img/lstm.png)\n",
    "\n",
    "**TODO**: explain cell (credit to [Shi Yan](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714))\n",
    "\n",
    "### 1.3 Further reading\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[The Deep Learning textbook](http://www.deeplearningbook.org/) (Chapter 10 is most relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toy example\n",
    "\n",
    "**TODO**: go through character-level RNN tutorial at https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo of larger pre-trained example\n",
    "\n",
    "**TODO @PENG**: add parameters and code for pre-trained language model, add some basic material about surprisal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
