{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks (RNNs) have become increasingly popular in machine learning for handling sequential data. In this tutorial, we will cover [background](#1.-Background) about the architecture, a toy [training example](#2.-Training-a-toy-model), and a demo for [evaluating a larger pre-trained model](#4.-Evaluating-a-pre-trained-model).\n",
    "\n",
    "**Date**: June 26, 2019\n",
    "\n",
    "**Authors**: Jennifer Hu (MIT), Ben Huh (IBM), Peng Qian (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "To run this tutorial, you will need to install [Pytorch](https://pytorch.org/). Just follow the instructions in the docs to install the library and its dependencies. To test your installation, go ahead and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need [numpy](https://www.numpy.org/), which comes automatically installed with a scientific Python distribution such as [Anaconda](https://www.anaconda.com/). Of course, you can also install it manually. Test your installation by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background: What is an RNN?\n",
    "\n",
    "Note: The following figures were taken from excellent blog posts by [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Jianqiang Ma](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e), and [Shi Yan](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714).\n",
    "\n",
    "In both machine learning and everyday life, many tasks make use of **sequential** data. Take language understanding, for example. As you comprehend each word in this sentence, you draw upon information from the previous words. RNNs have an **inductive bias** for handling this type of data, as parameters are shared across positions in the sequence.\n",
    "\n",
    "How does this architecture compare to \"normal\" neural networks? Let $A$ be a neural network, $x_t$ the input, and $h_t$ the output or hidden state vector. Consider the following diagram (credit to [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
    "\n",
    "![RNN](img/rnn_unrolled.png)\n",
    "\n",
    "The self-loop at $A$ represents the **recurrence** of the network. We can \"unroll\" it into multiple copies of the same network $A$, sending information down the temporal chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a toy RNN\n",
    "\n",
    "Let's try training our own toy RNN.\n",
    "Here, we implement the model from the following paper:\n",
    "\n",
    "> \"Task representations in neural networks trained to perform many cognitive tasks.\" Guangyu Robert Yang, Madhura R. Joglekar, H. Francis Song, William T. Newsome & Xiao-Jing Wang (2019). [*Nature Neuroscience* Volume 22, pp. 297â€“306](https://www.nature.com/articles/s41593-018-0310-2).\n",
    "\n",
    "This will train a RNN model for 20 types of cognitive tasks. Here's an example: \n",
    "\n",
    "<img src =  \"img/XJW.png\"     style=\"text-align:center; width: 400px;\"/>\n",
    "\n",
    "The original code can be found [at this repo](https://github.com/gyyang/multitask).\n",
    "\n",
    "We will be implementing a **rate-neuron network model**, which is described by\n",
    "\n",
    "$$ \\tau \\dot{h} = - h + \\sigma(W_{hh} h + W_{ih} s) $$\n",
    "\n",
    "where $x$ is neural (hidden) state,  $s$ is (sensory) input, and $W_{hh}, W_{ih}$ are synaptic weight parameters (recurrent and input weights).\n",
    "\n",
    "Note that this model uses continuous-time dynamics description, $\\dot{h} = f(h(t))$, \n",
    "whereas deep-learning models use discrete-time dynamics: $h_{t+1} = f(h_{t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from RNNCell_base import * \n",
    "                        \n",
    "class RNNCell(RNNCell_base):\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity = None, bias = True, decay = 0.9):\n",
    "        super().__init__(input_size, hidden_size, nonlinearity, bias)\n",
    "        self.decay = decay    #  torch.exp( - dt/tau )\n",
    "\n",
    "    def forward(self, input, hidden):                        \n",
    "        activity = self.nonlinearity(input @ self.weight_ih.t() +  hidden @ self.weight_hh.t() + self.bias)\n",
    "        hidden   = self.decay * hidden + (1 - self.decay) * activity\n",
    "#         hidden   = activity       # for discrete-time model (decay = 0)\n",
    "        return hidden \n",
    "\n",
    "class RNNLayer(nn.Module): \n",
    "    def __init__(self, RNNcell, *args):\n",
    "        super().__init__()\n",
    "        self.RNNcell = RNNcell(*args)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        inputs = input.unbind(0)\n",
    "        state = state[0]\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            state = self.RNNcell(inputs[i], state)\n",
    "            outputs += [state]\n",
    "        return torch.stack(outputs), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running a RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, batch = 3000, 100\n",
    "n_input, n_rnn, n_output = 10, 500, 5\n",
    "\n",
    "x       = torch.randn(T, batch, n_input)\n",
    "hidden0 = torch.zeros([1, batch, n_rnn])\n",
    "\n",
    "rnn  = RNNLayer(RNNCell, n_input, n_rnn, nn.ReLU(), 0.9, True)\n",
    "# rnn  = nn.RNN(n_input, n_rnn, nonlinearity = 'relu')\n",
    "# rnn  = nn.LSTM(n_input, n_rnn, nonlinearity = 'relu')\n",
    "\n",
    "output, hidden = rnn(x, hidden0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple network model that consists of a recurrent layer and a readout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hp): \n",
    "        super().__init__()\n",
    "        n_input, n_rnn, n_output, decay = hp['n_input'], hp['n_rnn'], hp['n_output'], hp['decay']\n",
    "        \n",
    "        if hp['activation'] == 'relu':    # Type of activation runctions, relu, softplus, tanh, elu\n",
    "            nonlinearity = nn.ReLU()\n",
    "        else: \n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self.n_rnn = n_rnn\n",
    "        self.rnn   = RNNLayer(RNNCell, n_input, n_rnn, nonlinearity, decay)\n",
    "        self.readout = nn.Linear(n_rnn, n_output, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden0   = torch.zeros([1, x.shape[1], self.n_rnn])\n",
    "        hidden, _ = self.rnn(x, hidden0)\n",
    "        output    = self.readout(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main training loop, for the 'Mante' task, which trains for mixed motion and color cues, implemented in the file `train_pytorch.py`. Over time, cost should go down and performance should go up.\n",
    "\n",
    "<img src =  \"img/Mante_task.png\"     style=\"text-align:center; width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial       0  | Time 0.00 s  | Now training contextdm1 & contextdm2\n",
      "contextdm1     | cost 0.592275| c_reg 0.000000  | perf 0.18\n",
      "contextdm2     | cost 0.736859| c_reg 0.000000  | perf 0.29\n",
      "Trial   32000  | Time 167.04 s  | Now training contextdm1 & contextdm2\n",
      "contextdm1     | cost 0.212513| c_reg 0.000000  | perf 0.47\n",
      "contextdm2     | cost 0.209913| c_reg 0.000000  | perf 0.49\n",
      "Trial   64000  | Time 330.33 s  | Now training contextdm1 & contextdm2\n",
      "contextdm1     | cost 0.168450| c_reg 0.000000  | perf 0.63\n",
      "contextdm2     | cost 0.170018| c_reg 0.000000  | perf 0.60\n"
     ]
    }
   ],
   "source": [
    "from train_pytorch import set_hyperparameters, train, Run_Model\n",
    "hp, log, optimizer  = set_hyperparameters(model_dir='debug', hp={'learning_rate': 0.001}, ruleset='mante')\n",
    "# rnn_model = RNN_Model(hp, nn.ReLU(), 0.9)\n",
    "model = Model(hp)\n",
    "run_model = Run_Model(hp, model)\n",
    "train(run_model, optimizer, hp, log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import standard_analysis\n",
    "rule = 'contextdm1'\n",
    "standard_analysis.easy_activity_plot(run_model, rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic RNN model worked well for this example, but in practice, many applications use a special type of RNN called a **long short-term memory (LSTM)** network. From now on, we'll call non-LSTM RNNs \"vanilla RNNs\".\n",
    "\n",
    "## 3. LSTMs: Beyond vanilla RNNs\n",
    "\n",
    "To motivate the popularity of LSTMs, recall why we were interested in RNNs in the first place: modeling *sequences*. The idea is that items at later parts of a sequence can depend on items at earlier parts of the sequence. The distance between these related items is often called the **dependency length**.\n",
    "\n",
    "Dependency lengths can be quite short in some cases and quite long in others, even for the same task. Returning to the language example, suppose your task is to predict the last word in a sentence. If your sentence is \n",
    "\n",
    "> A car has four *wheels*.\n",
    "\n",
    "then the gap between the target word (here, \"wheels\") and the relevant parts of the sequence (here, \"car\") is relatively small. In contrast, consider the sentence \n",
    "\n",
    "> A car can have sentimental value for many owners for a variety of reasons, and can come in many models, sizes, and colors; nevertheless, one defining characteristic of such a machine is that it has four *wheels*.\n",
    "\n",
    "Here, the gap between \"wheels\" and \"car\" is relatively large. Words closer to the end of the sentence such as \"machine\" and \"four\" can help guide the prediction of \"wheels\", but we need the word \"car\" to nail down the correct word.\n",
    "\n",
    "In theory, vanilla RNNs are able to capture these dependencies of any length, but in practice, they often fail on long-term dependencies ([Bengio et al. 1994](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf)). LSTMs ([Hochreiter & Schmidhuber 1997](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735)) turn out to be an effective solution to this problem.\n",
    "\n",
    "### 3.1 LSTM architecture\n",
    "\n",
    "Consider the following diagram of an LSTM (credit to [Shi Yan](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)):\n",
    "\n",
    "![LSTM](img/lstm.png)\n",
    "\n",
    "There's a lot going on here, but in short, LSTMs have three multiplicative gates:\n",
    "\n",
    "1. The **input gate** controls what proportion of the input to pass to the memory cell.\n",
    "2. The **forget gate** controls what proportion of the previous memory cell information to discard.\n",
    "3. The **output gate** controls what proportion of the memory cell to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating a pre-trained model\n",
    "\n",
    "Now that we've learned how to train a small RNN and have learned about LSTMs, let's examine the behavior of an LSTM that has already been pre-trained on a larger amount of data.\n",
    "\n",
    "In particular, we will be playing around with a **language model**. A language model assigns a probability distribution over words. It is trained to perform the following task: at each point in a sentence, predict the most probable next word given the preceding context. For example, if the first word of a sentence is \"The\", then a reasonable language model might assign high probabilities to nouns (e.g. \"dog\" or \"boy\") and low probabilities to verbs (e.g. \"swims\" or \"sings\") as the next word.\n",
    "\n",
    "How can we evaluate whether a language model has successfully \"learned\" English grammar, aside from using our intuition? One thing we can do is test the language model on some basic grammatical phenomena, such as **subject-verb agreement.**\n",
    "\n",
    "### 4.1 Subject-verb agreement\n",
    "\n",
    "In English, every well-formed sentence has a **subject** and a **verb**. For example, in the sentence \n",
    "\n",
    "> (1) The dog sniffs a bone.\n",
    "\n",
    "*dog* is the subject, and *sniffs* is the verb. The subject and verb must **agree** in **number** -- that is, a singular subject must be paired with a singular verb, and a plural subject must be paired with a plural verb. Returning to our example, this simply means that\n",
    "\n",
    "> (1) The dog sniffs a bone.\n",
    ">\n",
    "> (2) The dogs sniff a bone.\n",
    "  \n",
    "are both grammatically well-formed, but \n",
    "\n",
    "> (3) \\*The dog sniff a bone.\n",
    ">\n",
    "> (4) \\*The dogs sniffs a bone.\n",
    "  \n",
    "are not. (That's what the asterisk means!)\n",
    "\n",
    "### 4.2 Evaluation metric\n",
    "\n",
    "How does this relate to LSTM language models? The key assumption is that a language model that has successfully \"learned\" English should assign **higher probability** to grammatically well-formed sentences (\"grammatical\"), and **lower probability** to grammatically ill-formed sentences (\"ungrammatical\").\n",
    "\n",
    "This gives us a metric for evaluating our language model. Recall that language models assign probabilities at the level of each word. So a successful model should assign\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{singular verb }|\\text{ singular subject}) &> P(\\text{plural verb }|\\text{ singular subject}) \\\\\n",
    "P(\\text{plural verb }|\\text{ plural subject}) &> P(\\text{singular verb }|\\text{ plural subject}).\n",
    "\\end{align*}\n",
    "\n",
    "Returning to our example, this means that we would expect\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{sniffs }|\\text{ The dog}) &> P(\\text{sniff }|\\text{ The dog}) \\\\\n",
    "P(\\text{sniff }|\\text{ The dogs}) &> P(\\text{sniffs }|\\text{ The dogs}).\n",
    "\\end{align*}\n",
    "\n",
    "Finally, for reasons beyond the scope of this tutorial, psycholinguists are typically interested in the **negative log probability** of words, as [it is better correlated with human behavioral measures](https://www.mit.edu/~rplevy/papers/smith-levy-2013-cognition.pdf). Since this quantity is known as **surprisal**, let's denote it by $S$. This means that we will equivalently be looking for the pattern \n",
    "\n",
    "\\begin{align*}\n",
    "S(\\text{sniffs }|\\text{ The dog}) &< S(\\text{sniff }|\\text{ The dog}) \\\\\n",
    "S(\\text{sniff }|\\text{ The dogs}) &< S(\\text{sniffs }|\\text{ The dogs}).\n",
    "\\end{align*}\n",
    "\n",
    "So, does this actually hold in practice? Let's take a look below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some helper functions from `utils.py` that we'll need for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to download the parameters for the pre-trained model. This language model has been trained on an Americanized version of the [British National Corpus (BNC)](https://www.english-corpora.org/bnc/). You can **download the model checkpoint** [here](https://www.dropbox.com/s/er9exdbwun4rex9/model_bnc.pt?dl=1). Clicking on the link should download a file called `model_bnc.pt`, which you should move to the `materials` folder.\n",
    "\n",
    "Once you've done that, let's set some parameters and paths to important information: the model checkpoint, input sentences, and dictionaries containing vocabulary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'checkpoint': 'materials/model_bnc.pt', # path to model checkpoint\n",
    "    'eval_data': 'materials/examples.txt', # path to eval data\n",
    "    'dicts': 'materials/lstm_bnc_dicts.json', # path to dictionaries\n",
    "    'temperature': 1.0, # higher temperature will increase diversity\n",
    "    'seed': 1111 # random seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll initialize some settings, e.g. random seed and device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(params['seed'])\n",
    "device = torch.device(\"cpu\")\n",
    "assert(params['temperature'] >= 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and initialize the model from the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(68344, 400)\n",
       "  (rnn): LSTM(400, 400, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=400, out_features=68344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(params['checkpoint'], 'rb') as f:\n",
    "    model = torch.load(f, map_location=lambda storage, loc: storage)\n",
    "    model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sentences and dictionaries from the specified paths. Note that the sentence file `params['eval_data']` is expected to have one sentence on each line, with tokens separated by spaces. (For the simple sentences that we'll be working with, just think of separating each word with a space, as well as the final period.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eval sentences\n",
    "with open(params['eval_data'], 'r') as f:\n",
    "    lines = f.readlines()\n",
    "sents = [line.strip().split() for line in lines]\n",
    "\n",
    "# load the dictionaries, word2idx and idx2word, of the preprocessed corpus used by the model\n",
    "corpus_dict = utils.Dictionary()\n",
    "corpus_dict.load(params['dicts'])\n",
    "ntokens = len(corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to preprocess the eval data in two ways. First, since there may be words in the eval data that the model has not seen during training, we need to map out-of-vocabulary words to fine-grained `UNK` tokens. Second, we add an `<eos>` token to signify the beginning of the sentence. Otherwise, the model would not be able to assign a probability to the first word, since there is no explicit preceding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map out-of-vocabulary words to \"UNK\" tokens\n",
    "sents = [utils.unkify(sent, corpus_dict.word2idx) for sent in sents]\n",
    "\n",
    "# add <eos> token to beginning of each sentence\n",
    "for sent in sents:\n",
    "    sent.insert(0, '<eos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to perform the evaluation. Each word and its negative log probability will be printed on a new line, separated by a tab (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: <eos> The dog sniffs a bone .\n",
      "<eos>\t--\n",
      "The\t2.4970164\n",
      "dog\t8.273961\n",
      "sniffs\t11.779587\n",
      "a\t2.979677\n",
      "bone\t7.6778884\n",
      ".\t2.6759293\n",
      "\n",
      "SENTENCE: <eos> The dogs sniff a bone .\n",
      "<eos>\t--\n",
      "The\t2.4970164\n",
      "dogs\t10.054913\n",
      "sniff\t11.877056\n",
      "a\t3.480141\n",
      "bone\t8.105723\n",
      ".\t2.6828048\n",
      "\n",
      "SENTENCE: <eos> The dog sniff a bone .\n",
      "<eos>\t--\n",
      "The\t2.4970164\n",
      "dog\t8.273961\n",
      "sniff\t13.05693\n",
      "a\t3.4009051\n",
      "bone\t7.726699\n",
      ".\t2.6417565\n",
      "\n",
      "SENTENCE: <eos> The dogs sniffs a bone .\n",
      "<eos>\t--\n",
      "The\t2.4970164\n",
      "dogs\t10.054913\n",
      "sniffs\t14.093539\n",
      "a\t3.5034008\n",
      "bone\t8.180921\n",
      ".\t2.7263448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sent in sents:\n",
    "        # print the sentence\n",
    "        print('SENTENCE:', ' '.join(sent))\n",
    "        \n",
    "        # initialize the hidden layer\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        # map the first word of the sentence to its index in the model vocabulary\n",
    "        input = torch.tensor([[corpus_dict.word2idx[sent[0]]]],dtype=torch.long).to(device)\n",
    "        print(sent[0]+'\\t'+'--')\n",
    "\n",
    "        for i, w in enumerate(sent[1:]):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = torch.Tensor.numpy(output.squeeze().div(params['temperature']).exp().cpu())\n",
    "            total_weight = np.sum(word_weights)\n",
    "            word_idx = corpus_dict.word2idx[w]\n",
    "            word_surprisal = -np.log(word_weights[word_idx]/total_weight)\n",
    "            print(w+'\\t'+str(word_surprisal))\n",
    "            input.fill_(word_idx)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see \n",
    "\n",
    "\\begin{align*}\n",
    "S(\\text{sniffs }|\\text{ $<$eos$>$ The dog}) = 11.78 &< S(\\text{sniff }|\\text{ $<$eos$>$The dog}) = 13.05 \\;\\; \\checkmark \\\\\n",
    "S(\\text{sniff }|\\text{ $<$eos$>$ The dogs}) = 11.88 &< S(\\text{sniffs }|\\text{ $<$eos$>$The dogs}) = 14.09 \\;\\; \\checkmark\n",
    "\\end{align*}\n",
    "\n",
    "as expected. This suggests that the language model has indeed learned something about subject-verb number agreement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Exploration\n",
    "\n",
    "Now, it's your turn to play around with the model by designing your own test sentences. You can edit `materials/examples.txt` directly and re-run all the cells after loading the eval data, or make your own file following the format of `materials/examples.txt` and update `params['eval_data']` accordingly. \n",
    "\n",
    "Do the results surprise you? What other grammatical phenomena would be good for assessing language models?\n",
    "\n",
    "(Note: if you're interested in training a language model yourself, see [this Pytorch example](https://github.com/pytorch/examples/tree/master/word_language_model).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Useful resources\n",
    "\n",
    "### 5.1 Further reading\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[The Deep Learning textbook](http://www.deeplearningbook.org/) (Chapter 10 is most relevant)\n",
    "\n",
    "### 5.2 Further practice\n",
    "\n",
    "[Pytorch character generation tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
    "\n",
    "[Pytorch character classification tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
